{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sqlalchemy import create_engine\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1. preliminary load-n-scrub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cnx():\n",
    "    #*********************************************REPLACE WITH YOUR USERNAME:PASSWORD\n",
    "    cnx = create_engine('postgresql://username:password@54.236.113.118:5432/jaysips')\n",
    "    return cnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_datasets(cnx):\n",
    "    '''Retrieves datasets from psql.\n",
    "    '''\n",
    "    cleveland = pd.read_sql_query('''SELECT * FROM cleveland''', cnx)\n",
    "    hungary = pd.read_sql_query('''SELECT * FROM hungary''', cnx)\n",
    "    longbeach = pd.read_sql_query('''SELECT * FROM longbeach''', cnx)\n",
    "    swiss = pd.read_sql_query('''SELECT * FROM swiss''', cnx)\n",
    "    datasets = {'Cleveland Clinic': cleveland, 'Hungarian Institute of Cardiology': hungary, \n",
    "            'Swiss University Hospitals': swiss, 'Long Beach V.A. Medical Center': longbeach}\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_datasets(datasets, binary=True, drop_above=0.9, avg_below=0.1):\n",
    "    cleaned = {}    \n",
    "    for name, hosp in datasets.iteritems():\n",
    "        \n",
    "        # Getting rid of irrelevant rows\n",
    "        hosp = hosp.iloc[:, :58]\n",
    "        if binary == True:\n",
    "            # Making heart disease feature binary 0/1\n",
    "            hosp['num'] = hosp['num'].replace(2, 1).replace(3, 1).replace(4, 1)\n",
    "        # Dropping rows that are unfilled (-9s) in every dataset\n",
    "        hosp = hosp.drop(hosp.iloc[:, 44:50], axis=1).drop(hosp.iloc[:, 51:54], axis=1)\n",
    "        hosp = hosp.drop('pncaden', axis=1).drop('dm', axis=1)\n",
    "        # Replacing -9s with NaNs\n",
    "        hosp = hosp.replace(-9, np.nan)    \n",
    "        \n",
    "        for col in hosp:\n",
    "            nans = hosp[col].isnull().sum()\n",
    "            # Dropping cols in a given dataset that are (default) > 90% NaNs\n",
    "            if nans > len(hosp)*drop_above:\n",
    "                hosp = hosp.drop(col, axis=1)\n",
    "            # Replacing NaNs with column means for cols that are (default) < 10% NaNs \n",
    "            elif nans > 0 and nans < len(hosp)*avg_below:            \n",
    "                hosp[col] = hosp[col].replace(np.nan, np.mean(hosp[col]))\n",
    "            else:\n",
    "                continue  \n",
    "                \n",
    "        cleaned[name] = hosp   \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned = clean_datasets(get_datasets(get_cnx()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. assessing the NaN situ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_nans(cleaned):\n",
    "    '''Get dict of cols missing vals for each dataset.'''\n",
    "    NaNs = {}\n",
    "    for name, hosp in cleaned.iteritems():\n",
    "        nulls = hosp.isnull().sum()\n",
    "        for col, n in nulls.iteritems():\n",
    "            if n == 0:\n",
    "                nulls = nulls.drop(col)\n",
    "        NaNs[name] = nulls\n",
    "    return NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for name, hosp in cleaned.iteritems():\n",
    "#     print name, len(hosp), len(hosp.columns)\n",
    "# NaNs = view_nans(cleaned)\n",
    "# NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #What % of cols are missing values in each dataset? \n",
    "# for name, hosp in cleaned.iteritems():\n",
    "#     print name, float(len(NaNs[name]))/float(len(hosp.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NaNs = view_nans(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3. using KNN to estimate missing values\n",
    "(going in order from cols with least NaNs to those with most, by dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for `estimate_missing_values`:\n",
    "- `df_nans_to_mean`\n",
    "- `knn_best_k`\n",
    "- `train_test_split_nonnan_nan`\n",
    "- `knn_fillna`\n",
    "- `to_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_nans_to_mean(df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for col in df:\n",
    "        new_df[col] = df[col].replace(np.nan, np.mean(df[col]))\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_best_k(X, y):\n",
    "    recall_scores = {}\n",
    "    for k in range(1,21):\n",
    "        if len(y.unique()) <= 12:\n",
    "            #print \"in classifier\"\n",
    "            knn_model = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "            y = y.apply(str)\n",
    "            cv_score = cross_val_score(knn_model, X, y, cv=5, scoring='recall_micro')\n",
    "        else:\n",
    "            #print \"in regressor\"\n",
    "            knn_model = KNeighborsRegressor(n_neighbors=k, weights='distance') \n",
    "            cv_score = cross_val_score(knn_model, X, y, cv=5, scoring='r2')\n",
    "        gen_recall = np.mean(cv_score)   \n",
    "        recall_scores[k] = gen_recall  \n",
    "    best_k = max(recall_scores.iteritems(), key=itemgetter(1))[0]\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split_nonnan_nan(df, y):\n",
    "    '''Split up a df into train/test sets according to absence/presence of missing values\n",
    "    in the dependent variable column, y, where y = column name.'''\n",
    "    estimations = df_nans_to_mean(df)\n",
    "    train = estimations[df[y].notnull()]  #df_nans_to_mean(df[df[y].notnull()])\n",
    "    test = estimations[df[y].isnull()]    #df_nans_to_mean(df[df[y].isnull()])\n",
    "    #print len(train)\n",
    "    #print len(test)\n",
    "    X_train, X_test, y_train, y_test = train.drop(y, axis=1), test.drop(y, axis=1), train[y], test[y]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_fillna(k, X_train, X_test, y_train):\n",
    "    '''Takes in training data, spits out y values...'''\n",
    "    if len(y_train.unique()) <= 12:\n",
    "        knn_model = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "    else:\n",
    "        knn_model = KNeighborsRegressor(n_neighbors=k, weights='distance') \n",
    "    knn_model.fit(X_train, y_train) \n",
    "    y_pred = knn_model.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_dict(y_est, y_nans):\n",
    "    '''Turns y_estimates into a dict with y_nans indices as keys.'''\n",
    "    y_dict = {}\n",
    "    for index, _ in y_nans.iteritems():\n",
    "        for i in range(len(y_nans)):\n",
    "            y_dict[index] = y_est[i]\n",
    "    return y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# full thang\n",
    "def estimate_missing_values(cleaned, NaNs):\n",
    "    '''\n",
    "    Input: cleaned, dict of cleaned df; NaNs, dict of missing values by dataset\n",
    "    Output: polished dict of datasets with no missing values!\n",
    "    '''\n",
    "    polished = {}\n",
    "    for name, hosp in cleaned.iteritems():\n",
    "        for col, _ in tqdm.tqdm(NaNs[name].sort_values().iteritems()):\n",
    "            # Find best k:\n",
    "            X = df_nans_to_mean(hosp.drop(col, axis=1)) \n",
    "            y = hosp[col].fillna(value=np.mean(hosp[col]))\n",
    "            k_neighbors = knn_best_k(X, y)\n",
    "            # Get \"training\" (non-nan) and \"testing\" (nan) sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split_nonnan_nan(hosp, col)\n",
    "            # Estimate missing values w/ knn:\n",
    "            y_estimates = knn_fillna(k_neighbors, X_train, X_test, y_train)\n",
    "            # Fill missing values with knn estimates\n",
    "            y_estimates = to_dict(y_estimates, y_test)\n",
    "            hosp[col] = hosp[col].fillna(value=y_estimates)\n",
    "        polished[name] = hosp\n",
    "    return polished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "preprocessed_datasets = estimate_missing_values(cleaned, NaNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`met` and `proto` may throw you warnings...ignore them\n",
    "\n",
    "also unfortunately not yet able to round categoricals\n",
    "\n",
    "also Hungarian `proto` numbers are fuuuckd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preprocessed_datasets['Hungarian Institute of Cardiology']['met']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preprocessed_datasets['Hungarian Institute of Cardiology']['proto']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
